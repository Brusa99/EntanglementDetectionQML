{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Neural Network for entanglement detection\n",
    "\n",
    "In this notebook we are going to use fully connected neural network for the task of entanglement detection using observable mean values as features.\\\n",
    "The dataset is imported with pandas and the network is implemented with the torch library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We use the dataset of the quantum state: the ```ds_haar_op```.\n",
    "It's composed by 10 features and 1 label, 0 or 1 if the state is entangled or separable.\\\n",
    "The 10 features represent the means values of some observables (see the dataset folder for an extensive explanation of how the dataset is generated).\n",
    "\n",
    "\n",
    "Then the dataset is split in train and test part using the hypeparameters define below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters value\n",
    "\n",
    "N_DATA = 1000\n",
    "N_TRAIN = 800\n",
    "N_TEST = N_DATA - N_TRAIN\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "dataset_U = pd.read_csv('../datasets/ds_haar_obs.csv') # import the dataset\n",
    "dataset_U = dataset_U[:N_DATA] # select the number of data\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "\n",
    "# Drop the 11th column\n",
    "X = dataset_U.drop(columns=dataset_U.columns[10])\n",
    "X = np.array(X)\n",
    "X = th.tensor(X, dtype=th.float32)\n",
    "\n",
    "y = dataset_U.iloc[:, 5]  # Assuming the label is in the 11th column (index 10)\n",
    "y = th.tensor(y.to_numpy(dtype=int)) # convert the labels in torch tensor\n",
    "\n",
    "X = X[:,0:3] # select only some of the features\n",
    "\n",
    "#train\n",
    "obs_train = X[:N_TRAIN]\n",
    "y_train = y[:N_TRAIN]\n",
    "\n",
    "train_mapped_dataset = TensorDataset(obs_train,y_train) # create dataset\n",
    "train_mapped_loader = DataLoader(train_mapped_dataset, shuffle=True, batch_size=BATCH_SIZE) # create dataloader for the training\n",
    "\n",
    "\n",
    "#TEST\n",
    "obs_test = X[N_TRAIN:N_DATA]\n",
    "y_test = y[N_TRAIN:N_DATA]\n",
    "\n",
    "test_mapped_dataset = TensorDataset(obs_test,y_test) # create your datset\n",
    "test_mapped_loader = DataLoader(test_mapped_dataset, shuffle=False, batch_size=BATCH_SIZE) # create dataloader for the test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Define the model as an inherint class from the nn.Module of pytorch.\n",
    "It's define the constructor, i.e. the architecture of the network with:\n",
    "* input layer \n",
    "* hidden layer \n",
    "* output layer\n",
    "\n",
    "Then is the define a function for the forward part of the training in which is applied an activaction function to each layer and return the ouput of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model through a class:\n",
    "\n",
    "#!!! WARNING: Change in the input layer the number of features\n",
    "\n",
    "class Net_cluster1(nn.Module):\n",
    "\n",
    "    # define the model with the constructor of the class\n",
    "    def __init__(self):\n",
    "        super(Net_cluster1, self).__init__() \n",
    "        self.l1 = nn.Linear(in_features = 3 , out_features = 15) # input layer\n",
    "        self.l2 = nn.Linear(in_features = 15, out_features = 5) # hidden layer\n",
    "        self.term = nn.Linear(in_features = 5, out_features = 2) # output layer\n",
    "\n",
    "    # this function is used for the forward training part\n",
    "    def forward(self, x: th.Tensor) -> th.Tensor:\n",
    "        \n",
    "        x : th.Tensor = x.flatten(start_dim=1)\n",
    "        x : th.Tensor = F.relu(self.l1(x))\n",
    "        x : th.Tensor  = F.relu(self.l2(x))\n",
    "        #x : Tensor = F.relu(self.l3(x))\n",
    "        logits : th.Tensor = self.term(x)\n",
    "        out: th.Tensor = F.softmax(input=logits, dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_accuracy(logit, target):\n",
    "    \"\"\"\n",
    "    Obtain accuracy for one batch of data\n",
    "    Input:\n",
    "        - logit(torch.tensor): The predictions from the model \n",
    "        - target(torch.tensor): The y true values \n",
    "    Return:\n",
    "        - accuracy(float): The value of the accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    corrects = (th.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def train_loop(model, train_loader, EPOCHS, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function for training the model.\n",
    "    Input:\n",
    "        - model(torch model): The neural network\n",
    "        - train_loader(torch DataLoader): The train dataset passed as a torch DataLoader\n",
    "        - EPOCHS(int): Number of epochs for the training\n",
    "        - optimizer(torch optimizer): Optimizer for the learning algorithm\n",
    "        - criterion(torch loss functions): The loss function for the learning algorithm\n",
    "    Return: Print the loss and train accuracy and train the model\n",
    "    \"\"\"\n",
    "\n",
    "    # do the training loop through the number of epochs\n",
    "    for epoch in trange(EPOCHS):\n",
    "        train_running_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        model = model.train()  # Set the model to training mode: relevant for dropout, batchnorm, etc.\n",
    "\n",
    "        # Actual (batch-wise) training step\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Forward pass + (automated) BackProp + Loss computation\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels) # calculate the loss\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the gradients to zero: otherwise they accumulate!\n",
    "            loss.backward()  # Backpropagation\n",
    "\n",
    "            # Update model params\n",
    "            optimizer.step()\n",
    "\n",
    "            train_running_loss += loss.detach().item()\n",
    "            train_acc += get_batch_accuracy(logits, labels)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss/i} | Train Accuracy: {train_acc/i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainig and test part \n",
    "\n",
    "Here some important things for the training are defined:\n",
    "* The model\n",
    "* The loss function\n",
    "* The optimizer\n",
    "\n",
    "Then is used the train_loop function for the training part and after is calculated the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_cluster1() # define the model as an object of the model class\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = th.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)  # the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b62a408fe94f228bde9a02e8eef487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.4066555391488893 | Train Accuracy: 96.62077596996245\n",
      "Epoch: 2 | Loss: 0.31459855977375906 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 3 | Loss: 0.31387713760398656 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 4 | Loss: 0.31374435479262 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 5 | Loss: 0.3136979116963803 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 6 | Loss: 0.313677508370002 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 7 | Loss: 0.3136673170947312 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 8 | Loss: 0.3136617357053506 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 9 | Loss: 0.3136585580840129 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 10 | Loss: 0.3136566995231619 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 11 | Loss: 0.3136555687133899 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 12 | Loss: 0.31365487766504585 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 13 | Loss: 0.31365445565819294 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 14 | Loss: 0.3136541910180461 | Train Accuracy: 100.12515644555694\n",
      "Epoch: 15 | Loss: 0.31365402540814447 | Train Accuracy: 100.12515644555694\n",
      "\n",
      "Test Accuracy: 100.50251256281408\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loop(model, train_mapped_loader, EPOCHS, optimizer, criterion) # training part\n",
    "print(\"\")\n",
    "\n",
    "# calculate the test accuracy\n",
    "test_acc = 0.0\n",
    "for i, (dens_matr, labels) in enumerate(test_mapped_loader):\n",
    "    outputs = model(dens_matr)\n",
    "    test_acc += get_batch_accuracy(outputs, labels)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
