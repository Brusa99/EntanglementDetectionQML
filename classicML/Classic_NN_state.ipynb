{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Neural Network for entanglement detection\n",
    "\n",
    "In this notebook we are going to use fully connected neural network for the task of entanglement detection using quantum states.\\\n",
    "The dataset is imported with pandas and the network is implemented with the torch library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import trange # for the loading bar in for loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We use the dataset of the quantum state: the ```ds_haar_op```.\n",
    "It's composed by 16 features and 1 label, 0 or 1 if the state is entangled or separable.\\\n",
    "The 16 features represent the entries of the unitary matrix that generate the state composed by two qubits: $\\ket{\\psi_i} = U_i \\ket{00} \\, \\, i = 1,..,N$ where $N$ is the number of data.\\\n",
    "Then the denisty matrices are used for the training. They are computed as outer produtct of the states\n",
    "\n",
    "$$ \\rho_i = \\ket{\\psi_i}\\bra{\\psi_i}.$$\n",
    "\n",
    "Then the dataset is split in train and test part using the hypeparameters define below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters value\n",
    "\n",
    "N_DATA = 2000\n",
    "N_TRAIN = 1600\n",
    "N_TEST = N_DATA - N_TRAIN\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "dataset_U = pd.read_csv(\"../datasets/ds_haar_op.csv\") # import the dataset\n",
    "dataset_U = dataset_U[:N_DATA] # select the number of data\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "\n",
    "# Drop the 17th column that is the label\n",
    "X = dataset_U.drop(columns=dataset_U.columns[16])\n",
    "\n",
    "y = dataset_U.iloc[:, 16]  # Assuming the label is in the 17th column (index 16)\n",
    "y = th.tensor(y.to_numpy(dtype=int)) # convert the labels in torch tensor\n",
    "\n",
    "dataset_psi_out = X.iloc[:, :4] # select only the first row of the unitary matrix, because it represent the output state after apply U to |00>\n",
    "dataset_psi_out_np = dataset_psi_out.to_numpy(dtype=np.csingle) # convert the dataset in numpy\n",
    "\n",
    "# compute the density matrices dataset as a torch tensor \n",
    "dens_matrices = th.tensor(np.array([(np.outer(dataset_psi_out_np[i], np.conj(dataset_psi_out_np[i]))).flatten() for i in range(N_DATA)])) \n",
    "\n",
    "# divide in real and imaginary part\n",
    "dens_matrices_real_imag = th.cat((th.real(dens_matrices), th.imag(dens_matrices)), dim = 1)\n",
    "\n",
    "# divide in test train part\n",
    "\n",
    "#train\n",
    "dens_matrices_train = dens_matrices_real_imag[:N_TRAIN]\n",
    "y_train = y[:N_TRAIN]\n",
    "\n",
    "train_mapped_dataset = TensorDataset(dens_matrices_train,y_train) # create dataset\n",
    "train_mapped_loader = DataLoader(train_mapped_dataset, shuffle=True, batch_size=BATCH_SIZE) # create dataloader for the training\n",
    "\n",
    "\n",
    "#TEST\n",
    "dens_matrices_test = dens_matrices_real_imag[N_TRAIN:N_DATA]\n",
    "y_test = y[N_TRAIN:N_DATA]\n",
    "\n",
    "test_mapped_dataset = TensorDataset(dens_matrices_test,y_test) # create your datset\n",
    "test_mapped_loader = DataLoader(test_mapped_dataset, shuffle=False, batch_size=BATCH_SIZE) # create dataloader for the test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Define the model as an inherint class from the nn.Module of pytorch.\n",
    "It's define the constructor, i.e. the architecture of the network with:\n",
    "* input layer \n",
    "* hidden layer \n",
    "* output layer\n",
    "\n",
    "Then is the define a function for the forward part of the training in which is applied an activaction function to each layer and return the ouput of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model through a class:\n",
    "\n",
    "class Net_cluster1(nn.Module):\n",
    "\n",
    "    # define the model with the constructor of the class\n",
    "    def __init__(self):\n",
    "        super(Net_cluster1, self).__init__() \n",
    "        self.l1 = nn.Linear(in_features = 16+16 , out_features = 50) # input layer\n",
    "        self.l2 = nn.Linear(in_features = 50, out_features = 15) # hidden layer\n",
    "        self.term = nn.Linear(in_features = 15, out_features = 2) # output layer\n",
    "\n",
    "    # this function is used for the forward training part\n",
    "    def forward(self, x: th.Tensor) -> th.Tensor:\n",
    "        \n",
    "        x : th.Tensor = x.flatten(start_dim=1)\n",
    "        x : th.Tensor = F.relu(self.l1(x))\n",
    "        x : th.Tensor  = F.relu(self.l2(x))\n",
    "        #x : Tensor = F.relu(self.l3(x))\n",
    "        logits : th.Tensor = self.term(x)\n",
    "        out: th.Tensor = F.softmax(input=logits, dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important function\n",
    "\n",
    "There are some fundamental function below for the learning of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_accuracy(logit, target):\n",
    "    \"\"\"\n",
    "    Obtain accuracy for one batch of data\n",
    "    Input:\n",
    "        - logit(torch.tensor): The predictions from the model \n",
    "        - target(torch.tensor): The y true values \n",
    "    Return:\n",
    "        - accuracy(float): The value of the accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    corrects = (th.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def train_loop(model, train_loader, EPOCHS, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function for training the model.\n",
    "    Input:\n",
    "        - model(torch model): The neural network\n",
    "        - train_loader(torch DataLoader): The train dataset passed as a torch DataLoader\n",
    "        - EPOCHS(int): Number of epochs for the training\n",
    "        - optimizer(torch optimizer): Optimizer for the learning algorithm\n",
    "        - criterion(torch loss functions): The loss function for the learning algorithm\n",
    "    Return: Print the loss and train accuracy and train the model\n",
    "    \"\"\"\n",
    "\n",
    "    # do the training loop through the number of epochs\n",
    "    for epoch in trange(EPOCHS):\n",
    "        train_running_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        model = model.train()  # Set the model to training mode: relevant for dropout, batchnorm, etc.\n",
    "\n",
    "        # Actual (batch-wise) training step\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Forward pass + (automated) BackProp + Loss computation\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels) # calculate the loss\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the gradients to zero: otherwise they accumulate!\n",
    "            loss.backward()  # Backpropagation\n",
    "\n",
    "            # Update model params\n",
    "            optimizer.step()\n",
    "\n",
    "            train_running_loss += loss.detach().item()\n",
    "            train_acc += get_batch_accuracy(logits, labels)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss/i} | Train Accuracy: {train_acc/i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainig and test part \n",
    "\n",
    "Here some important things for the training are defined:\n",
    "* The model\n",
    "* The loss function\n",
    "* The optimizer\n",
    "\n",
    "Then is used the train_loop function for the training part and after is calculated the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_cluster1() # define the model as an object of the model class\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = th.optim.Adam(params=model.parameters(), lr=LEARNING_RATE) # the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836cea2bce9e4c28a6cc2ab51ef34576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.7067818921439502 | Train Accuracy: 51.7219387755102\n",
      "Epoch: 2 | Loss: 0.7036765546214824 | Train Accuracy: 51.7219387755102\n",
      "Epoch: 3 | Loss: 0.6985502389012551 | Train Accuracy: 59.05612244897959\n",
      "Epoch: 4 | Loss: 0.6897046152426272 | Train Accuracy: 61.0969387755102\n",
      "Epoch: 5 | Loss: 0.6757097560532239 | Train Accuracy: 65.88010204081633\n",
      "Epoch: 6 | Loss: 0.6532498926532512 | Train Accuracy: 71.30102040816327\n",
      "Epoch: 7 | Loss: 0.6227319946094435 | Train Accuracy: 76.08418367346938\n",
      "Epoch: 8 | Loss: 0.588756433555058 | Train Accuracy: 81.12244897959184\n",
      "Epoch: 9 | Loss: 0.5584424715869281 | Train Accuracy: 83.6734693877551\n",
      "Epoch: 10 | Loss: 0.5313011274045828 | Train Accuracy: 86.4795918367347\n",
      "\n",
      "Test Accuracy: 91.66666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loop(model, train_mapped_loader, EPOCHS, optimizer, criterion) # training part\n",
    "print(\"\")\n",
    "\n",
    "# calculate the test accuracy\n",
    "test_acc = 0.0\n",
    "for i, (dens_matr, labels) in enumerate(test_mapped_loader):\n",
    "    outputs = model(dens_matr)\n",
    "    test_acc += get_batch_accuracy(outputs, labels)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
