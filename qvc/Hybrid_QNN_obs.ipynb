{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid model for entanglement detection\n",
    "\n",
    "In this notebook we are going to implement an hybrid model (a classic ml neural network with a quantum layer) on the observable dataset ```ds_haar_obs```.\\\n",
    "It's inspired by this [notebook](https://qiskit-community.github.io/qiskit-machine-learning/tutorials/05_torch_connector.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/rk87_q2x6q73135zrfmsjxgw0000gn/T/ipykernel_25204/301363067.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# necesserary functions and libraries are imported\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "import torch as th \n",
    "from torch import Tensor\n",
    "from torch.nn import CrossEntropyLoss, BCELoss, MSELoss\n",
    "from torch.optim import SGD\n",
    "\n",
    "# Additional torch-related imports\n",
    "from torch import cat, no_grad\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.nn import Module, Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit import QuantumCircuit \n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "\n",
    "from tqdm.auto import trange\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and hyperparameters\n",
    "\n",
    "We use the dataset of the quantum observables: the ```ds_haar_obs```.\n",
    "It's composed by 10 features and 1 label, 0 or 1 if the state is entangled or separable.\\\n",
    "The 10 features represent the means values of some observables (see the dataset folder for an extensive explanation of how the dataset is generated).\n",
    "\n",
    "\n",
    "Then the dataset is split in train and test part using the hypeparameters define below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DATA = 1000 # Total number of data\n",
    "N_TRAIN = 800 # Train data\n",
    "N_TEST = N_DATA - N_TRAIN # Test data\n",
    "BATCH_SIZE = 50 # Batch size\n",
    "EPOCHS = 5 # epochs number\n",
    "LEARNING_RATE = 0.001 # learning rate\n",
    "\n",
    "dataset_U = pd.read_csv('../datasets/ds_haar_obs.csv')\n",
    "dataset_U = dataset_U[:N_DATA]\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "\n",
    "# Drop the 10th column\n",
    "X = dataset_U.drop(columns=dataset_U.columns[10])\n",
    "X = np.array(X)\n",
    "X = th.tensor(X, dtype=th.float32)\n",
    "\n",
    "y = dataset_U.iloc[:, 10]  # Assuming the label is in the 17th column (index 16)\n",
    "y = th.tensor(y.to_numpy()).type(th.long) # convert the labels in torch tensor\n",
    "\n",
    "X = X[:,0:5] # Take a number of features to use\n",
    "\n",
    "\n",
    "#train\n",
    "obs_train = X[:N_TRAIN]\n",
    "y_train = y[:N_TRAIN]\n",
    "\n",
    "train_mapped_dataset = TensorDataset(obs_train,y_train) # create dataset\n",
    "train_mapped_loader = DataLoader(train_mapped_dataset, shuffle=True, batch_size=BATCH_SIZE) # create dataloader for the training\n",
    "\n",
    "\n",
    "#TEST\n",
    "obs_test = X[N_TRAIN:N_DATA]\n",
    "y_test = y[N_TRAIN:N_DATA]\n",
    "\n",
    "test_mapped_dataset = TensorDataset(obs_test,y_test) # create your datset\n",
    "test_mapped_loader = DataLoader(test_mapped_dataset, shuffle=False, batch_size=BATCH_SIZE) # create dataloader for the test\n",
    "\n",
    "num_inputs = obs_train.shape[1] #It will be both the number of inputs in the classical first successive, and the number of qubits in the quantum layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNN and Model\n",
    "\n",
    "The qnn is the quantum layer and it has two parts:\n",
    "* The encoding\n",
    "* The ansatz\n",
    "\n",
    "Define the whole model as an inherint class from the nn.Module of pytorch.\n",
    "It's define the constructor, i.e. the architecture of the network with:\n",
    "* input layer \n",
    "* two hidden layer \n",
    "* quantum layer\n",
    "* output layer\n",
    "\n",
    "Then is the define a function for the forward part of the training in which is applied an activaction function to each layer and return the ouput of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and create QNN\n",
    "def create_qnn(num_inputs):\n",
    "    feature_map = ZZFeatureMap(num_inputs)\n",
    "    ansatz = RealAmplitudes(num_inputs, reps=1)\n",
    "    qc = QuantumCircuit(num_inputs)\n",
    "    qc.compose(feature_map, inplace=True)\n",
    "    qc.compose(ansatz, inplace=True)\n",
    "\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = EstimatorQNN(\n",
    "        circuit=qc,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters,\n",
    "        input_gradients=True,\n",
    "    )\n",
    "    return qnn\n",
    "\n",
    "qnn4 = create_qnn(num_inputs) # the quantum model\n",
    "\n",
    "# Define torch NN module\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, qnn, num_inputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc0 = Linear(num_inputs, 40)\n",
    "        self.fc1 = Linear(40, 20)\n",
    "        self.fc2 = Linear(20, num_inputs)  # 2-dimensional input to QNN\n",
    "        self.qnn = TorchConnector(qnn)  # Apply torch connector, weights chosen\n",
    "        # uniformly at random from interval [-1,1].\n",
    "        self.fc3 = Linear(1, 1)  # 1-dimensional output from QNN\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.qnn(x)  # apply QNN\n",
    "        x = self.fc3(x)\n",
    "        out = F.softmax(input=x, dim=1)\n",
    "        #return out\n",
    "        return cat((out, 1 - out), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important function\n",
    "\n",
    "There are some fundamental function below for the learning of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_accuracy(logit, target):\n",
    "    \"\"\"\n",
    "    Obtain accuracy for one batch of data\n",
    "    Input:\n",
    "        - logit(torch.tensor): The predictions from the model \n",
    "        - target(torch.tensor): The y true values \n",
    "    Return:\n",
    "        - accuracy(float): The value of the accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    corrects = (th.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / target.size(0)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def train_loop(model, train_loader, EPOCHS, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function for training the model.\n",
    "    Input:\n",
    "        - model(torch model): The neural network\n",
    "        - train_loader(torch DataLoader): The train dataset passed as a torch DataLoader\n",
    "        - EPOCHS(int): Number of epochs for the training\n",
    "        - optimizer(torch optimizer): Optimizer for the learning algorithm\n",
    "        - criterion(torch loss functions): The loss function for the learning algorithm\n",
    "    Return: Print the loss and train accuracy and train the model\n",
    "        - train_history(list): List of values of the train error\n",
    "        - loss_history(list): List of values of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    train_history = []\n",
    "    loss_history = []\n",
    "\n",
    "    # do the training loop through the number of epochs\n",
    "    for epoch in trange(EPOCHS):\n",
    "        train_running_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        model = model.train()  # Set the model to training mode: relevant for dropout, batchnorm, etc.\n",
    "\n",
    "        # Actual (batch-wise) training step\n",
    "        for i, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "\n",
    "            \n",
    "            # Forward pass + (automated) BackProp + Loss computation\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the gradients to zero: otherwise they accumulate!\n",
    "            loss.backward()  # Backpropagation\n",
    "\n",
    "            # Update model params\n",
    "            optimizer.step()\n",
    "\n",
    "            train_running_loss += loss.detach().item()\n",
    "            train_acc += get_batch_accuracy(logits, labels)\n",
    "\n",
    "            if i%5 == 0:\n",
    "                print(\"iteration: \", (i))\n",
    "\n",
    "        loss_history.append(train_running_loss/i)\n",
    "        train_history.append(train_acc/i)\n",
    "                \n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        print(f\"Epoch: {epoch+1} | Loss: {train_running_loss/i} | Train Accuracy: {train_acc/i}\")\n",
    "\n",
    "    return train_history, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainig and test part \n",
    "\n",
    "Here some important things for the training are defined:\n",
    "* The model\n",
    "* The loss function\n",
    "* The optimizer\n",
    "\n",
    "Then is used the train_loop function for the training part and after is calculated the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Net(qnn4, num_inputs) # define the model as an object of the model class\n",
    "criterion = CrossEntropyLoss() # Loss function\n",
    "optimizer = th.optim.SGD(model4.parameters(), lr=  LEARNING_RATE)\n",
    "#optimizer = th.optim.Adam(params=model4.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history, loss_history =  train_loop(model4, train_mapped_loader, EPOCHS, optimizer, criterion) # training part\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the test accuracy\n",
    "test_acc = 0.0\n",
    "for i, (dens_matr, labels) in enumerate(test_mapped_loader):\n",
    "    outputs = model4(dens_matr)\n",
    "    \n",
    "    test_acc += get_batch_accuracy(outputs, labels)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc/(i+1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the train_error vs epochs and loss vs epochs\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "\n",
    "axs[0].plot(np.arange(EPOCHS), train_history)\n",
    "axs[0].set_title('Train Accuracy ')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(np.arange(EPOCHS), loss_history)\n",
    "axs[1].set_title('Loss function')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].grid()\n",
    "\n",
    "fig.suptitle('Training for the Hybrid NN with 5 features')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    predictions = model4(obs_test)\n",
    "\n",
    "\n",
    "# Convert predictions to numpy array and binarize (if needed)\n",
    "predictions_np = (predictions.numpy() > 0.5).astype(int)\n",
    "y_test_np = y_test.numpy()\n",
    "\n",
    "predictions_np  = predictions_np[:,1]\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_np, predictions_np)\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(conf_matrix, cmap='Blues', interpolation='nearest')\n",
    "\n",
    "# Add labels\n",
    "plt.title('Confusion Matrix Hybrid with observables')\n",
    "plt.xticks(np.arange(2), ['0', '1']) # Set x-axis ticks\n",
    "plt.yticks(np.arange(2), ['0', '1']) # Set y-axis ticks\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.colorbar(label='Counts')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predictions[:,1])\n",
    "\n",
    "# Calculate Area Under the ROC Curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve Hybrid with observables')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
