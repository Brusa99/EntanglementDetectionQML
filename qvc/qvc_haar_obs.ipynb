{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Variational Circuit\n",
    "In this notebook we will implement a quantum variational circuit (QVC), we use it as a binary classificator for detecting entanglement in a quantum state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates import IQPEmbedding, StronglyEntanglingLayers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset contains 10,000 examples consisting of 10 features and a binary label: _sep_. The feature are rappresente as observable generated from a determined quantum state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P11</th>\n",
       "      <th>P22</th>\n",
       "      <th>P33</th>\n",
       "      <th>P44</th>\n",
       "      <th>P13</th>\n",
       "      <th>P14</th>\n",
       "      <th>P24</th>\n",
       "      <th>P12</th>\n",
       "      <th>P23</th>\n",
       "      <th>P34</th>\n",
       "      <th>sep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.363562</td>\n",
       "      <td>0.363562</td>\n",
       "      <td>0.363562</td>\n",
       "      <td>0.363562</td>\n",
       "      <td>0.363562</td>\n",
       "      <td>0.363562</td>\n",
       "      <td>0.363562</td>\n",
       "      <td>0.363562</td>\n",
       "      <td>0.363562</td>\n",
       "      <td>0.363562</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.478920</td>\n",
       "      <td>0.478920</td>\n",
       "      <td>0.478920</td>\n",
       "      <td>0.478920</td>\n",
       "      <td>0.478920</td>\n",
       "      <td>0.478920</td>\n",
       "      <td>0.478920</td>\n",
       "      <td>0.478920</td>\n",
       "      <td>0.478920</td>\n",
       "      <td>0.478920</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.176573</td>\n",
       "      <td>0.067155</td>\n",
       "      <td>0.055469</td>\n",
       "      <td>0.062003</td>\n",
       "      <td>0.256511</td>\n",
       "      <td>0.179351</td>\n",
       "      <td>0.105687</td>\n",
       "      <td>0.311796</td>\n",
       "      <td>0.154631</td>\n",
       "      <td>0.023350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.139089</td>\n",
       "      <td>0.347255</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>0.201476</td>\n",
       "      <td>0.491367</td>\n",
       "      <td>0.382842</td>\n",
       "      <td>0.488343</td>\n",
       "      <td>0.233877</td>\n",
       "      <td>0.272572</td>\n",
       "      <td>0.224575</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0.337067</td>\n",
       "      <td>0.337067</td>\n",
       "      <td>0.337067</td>\n",
       "      <td>0.337067</td>\n",
       "      <td>0.337067</td>\n",
       "      <td>0.337067</td>\n",
       "      <td>0.337067</td>\n",
       "      <td>0.337067</td>\n",
       "      <td>0.337067</td>\n",
       "      <td>0.337067</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.131677</td>\n",
       "      <td>0.058190</td>\n",
       "      <td>0.077242</td>\n",
       "      <td>0.258419</td>\n",
       "      <td>0.300867</td>\n",
       "      <td>0.470797</td>\n",
       "      <td>0.429093</td>\n",
       "      <td>0.326760</td>\n",
       "      <td>0.131067</td>\n",
       "      <td>0.458097</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0.331434</td>\n",
       "      <td>0.377660</td>\n",
       "      <td>0.498875</td>\n",
       "      <td>0.234904</td>\n",
       "      <td>0.383674</td>\n",
       "      <td>0.139955</td>\n",
       "      <td>0.383059</td>\n",
       "      <td>0.422744</td>\n",
       "      <td>0.450552</td>\n",
       "      <td>0.230284</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.013663</td>\n",
       "      <td>0.044072</td>\n",
       "      <td>0.102192</td>\n",
       "      <td>0.122643</td>\n",
       "      <td>0.253487</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>0.129687</td>\n",
       "      <td>0.424721</td>\n",
       "      <td>0.033320</td>\n",
       "      <td>0.124894</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           P11       P22       P33       P44       P13       P14       P24  \\\n",
       "0     0.363562  0.363562  0.363562  0.363562  0.363562  0.363562  0.363562   \n",
       "1     0.478920  0.478920  0.478920  0.478920  0.478920  0.478920  0.478920   \n",
       "2     0.176573  0.067155  0.055469  0.062003  0.256511  0.179351  0.105687   \n",
       "3     0.022839  0.022839  0.022839  0.022839  0.022839  0.022839  0.022839   \n",
       "4     0.139089  0.347255  0.249997  0.201476  0.491367  0.382842  0.488343   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3995  0.375724  0.375724  0.375724  0.375724  0.375724  0.375724  0.375724   \n",
       "3996  0.337067  0.337067  0.337067  0.337067  0.337067  0.337067  0.337067   \n",
       "3997  0.131677  0.058190  0.077242  0.258419  0.300867  0.470797  0.429093   \n",
       "3998  0.331434  0.377660  0.498875  0.234904  0.383674  0.139955  0.383059   \n",
       "3999  0.013663  0.044072  0.102192  0.122643  0.253487  0.289485  0.129687   \n",
       "\n",
       "           P12       P23       P34    sep  \n",
       "0     0.363562  0.363562  0.363562   True  \n",
       "1     0.478920  0.478920  0.478920   True  \n",
       "2     0.311796  0.154631  0.023350  False  \n",
       "3     0.022839  0.022839  0.022839   True  \n",
       "4     0.233877  0.272572  0.224575  False  \n",
       "...        ...       ...       ...    ...  \n",
       "3995  0.375724  0.375724  0.375724   True  \n",
       "3996  0.337067  0.337067  0.337067   True  \n",
       "3997  0.326760  0.131067  0.458097  False  \n",
       "3998  0.422744  0.450552  0.230284  False  \n",
       "3999  0.424721  0.033320  0.124894  False  \n",
       "\n",
       "[4000 rows x 11 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of total exemples \n",
    "n_exemples = 4000\n",
    "\n",
    "# Loading dataset with pandas\n",
    "dataset = pd.read_csv('../datasets/ds_haar_obs.csv')\n",
    "dataset = dataset[:n_exemples]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features\n",
    "n_features = 4\n",
    "\n",
    "# Dividing dataset in features and label\n",
    "X = dataset.to_numpy()[:,0:n_features]\n",
    "y = dataset.to_numpy()[:,-1]\n",
    "\n",
    "# Scaling the inputs \n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Redifining labels on -1,1\n",
    "y_scaled = np.array(2 * (y - 0.5),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3200\n",
      "Test size: 800\n"
     ]
    }
   ],
   "source": [
    "# Spliting dataset in training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled,train_size=0.8)\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Qbits:  4\n"
     ]
    }
   ],
   "source": [
    "# Setting the number of qbits the same as the number of features\n",
    "n_qubits = len(X_train[0])\n",
    "print(\"Number of Qbits: \",n_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_var = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev_var, diff_method=\"parameter-shift\")\n",
    "def quantum_model(x, params):\n",
    "    \"\"\"A variational quantum model.\"\"\"\n",
    "\n",
    "    # embedding\n",
    "    IQPEmbedding(x, wires=range(n_qubits),n_repeats=2)\n",
    "\n",
    "    # trainable measurement\n",
    "    StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_model_plus_bias(x, params, bias):\n",
    "    \"\"\"Adding a bias.\"\"\"\n",
    "    return quantum_model(x, params) + bias\n",
    "\n",
    "def hinge_loss(predictions, targets):\n",
    "    \"\"\"Implements the hinge loss.\"\"\"\n",
    "    all_ones = torch.ones_like(targets)\n",
    "    hinge_loss = all_ones - predictions * targets\n",
    "    # trick: since the max(0,x) function is not differentiable,\n",
    "    # use the mathematically equivalent relu instead\n",
    "    hinge_loss = relu(hinge_loss)\n",
    "    return hinge_loss\n",
    "\n",
    "def quantum_model_train(n_layers, steps, batch_size):\n",
    "    \"\"\"Train the quantum model defined above.\"\"\"\n",
    "\n",
    "    params = np.random.random((n_layers, n_qubits, 3))\n",
    "    params_torch = torch.tensor(params, requires_grad=True)\n",
    "    bias_torch = torch.tensor(0.0)\n",
    "\n",
    "    opt = torch.optim.Adam([params_torch, bias_torch], lr=0.1)\n",
    "\n",
    "    loss_history = []\n",
    "    for i in range(steps):\n",
    "\n",
    "        batch_ids = np.random.choice(len(X_train), batch_size)\n",
    "\n",
    "        X_batch = X_train[batch_ids]\n",
    "        y_batch = y_train[batch_ids]\n",
    "\n",
    "        X_batch_torch = torch.tensor(X_batch, requires_grad=False)\n",
    "        y_batch_torch = torch.tensor(y_batch, requires_grad=False)\n",
    "\n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            preds = torch.stack(\n",
    "                [quantum_model_plus_bias(x, params_torch, bias_torch) for x in X_batch_torch]\n",
    "            )\n",
    "            loss = torch.mean(hinge_loss(preds, y_batch_torch))\n",
    "\n",
    "            # bookkeeping\n",
    "            current_loss = loss.detach().numpy().item()\n",
    "            loss_history.append(current_loss)\n",
    "            if i % 10 == 0:\n",
    "                print(\"step\", i, \", loss\", current_loss)\n",
    "\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        opt.step(closure)\n",
    "\n",
    "    return params_torch, bias_torch, loss_history\n",
    "\n",
    "\n",
    "def quantum_model_predict(X_pred, trained_params, trained_bias):\n",
    "    \"\"\"Predict using the quantum model defined above.\"\"\"\n",
    "\n",
    "    p = []\n",
    "    for x in X_pred:\n",
    "\n",
    "        x_torch = torch.tensor(x)\n",
    "        pred_torch = quantum_model_plus_bias(x_torch, trained_params, trained_bias)\n",
    "        pred = pred_torch.detach().numpy().item()\n",
    "        if pred > 0:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = -1\n",
    "\n",
    "        p.append(pred)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , loss 1.0149813402636358\n",
      "step 10 , loss 0.8521625866339153\n",
      "step 20 , loss 0.8684990630922765\n",
      "step 30 , loss 1.075010800341309\n"
     ]
    }
   ],
   "source": [
    "n_layers = 4\n",
    "batch_size = 20\n",
    "steps = 100\n",
    "trained_params, trained_bias, loss_history = quantum_model_train(n_layers, steps, batch_size)\n",
    "\n",
    "pred_test = quantum_model_predict(X_test, trained_params, trained_bias)\n",
    "print(\"accuracy on test set:\", accuracy_score(pred_test, y_test))\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
