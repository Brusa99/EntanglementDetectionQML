{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Variational Circuit\n",
    "In this notebook we will implement a quantum variational circuit (QVC), we use it as a binary classificator for detecting entanglement in a quantum state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates import IQPEmbedding, StronglyEntanglingLayers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset contains 10,000 examples consisting of 10 features and a binary label: _sep_. The feature are rappresente as observable generated from a determined quantum state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBS_0</th>\n",
       "      <th>OBS_1</th>\n",
       "      <th>OBS_2</th>\n",
       "      <th>OBS_3</th>\n",
       "      <th>OBS_4</th>\n",
       "      <th>OBS_5</th>\n",
       "      <th>OBS_6</th>\n",
       "      <th>OBS_7</th>\n",
       "      <th>OBS_8</th>\n",
       "      <th>OBS_9</th>\n",
       "      <th>separable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.268372</td>\n",
       "      <td>0.246112</td>\n",
       "      <td>0.320277</td>\n",
       "      <td>0.157779</td>\n",
       "      <td>0.255752</td>\n",
       "      <td>0.174061</td>\n",
       "      <td>0.301679</td>\n",
       "      <td>0.233190</td>\n",
       "      <td>0.208881</td>\n",
       "      <td>0.241814</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.241903</td>\n",
       "      <td>0.239770</td>\n",
       "      <td>0.258264</td>\n",
       "      <td>0.253273</td>\n",
       "      <td>0.243815</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.223547</td>\n",
       "      <td>0.237721</td>\n",
       "      <td>0.237809</td>\n",
       "      <td>0.241504</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.185326</td>\n",
       "      <td>0.223879</td>\n",
       "      <td>0.240827</td>\n",
       "      <td>0.249868</td>\n",
       "      <td>0.232626</td>\n",
       "      <td>0.242109</td>\n",
       "      <td>0.246553</td>\n",
       "      <td>0.233927</td>\n",
       "      <td>0.234095</td>\n",
       "      <td>0.246902</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.233334</td>\n",
       "      <td>0.252164</td>\n",
       "      <td>0.278026</td>\n",
       "      <td>0.249751</td>\n",
       "      <td>0.236253</td>\n",
       "      <td>0.240901</td>\n",
       "      <td>0.245941</td>\n",
       "      <td>0.248044</td>\n",
       "      <td>0.280448</td>\n",
       "      <td>0.269990</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.234331</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.250295</td>\n",
       "      <td>0.248989</td>\n",
       "      <td>0.247575</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.250756</td>\n",
       "      <td>0.240557</td>\n",
       "      <td>0.257094</td>\n",
       "      <td>0.250061</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0.213068</td>\n",
       "      <td>0.241627</td>\n",
       "      <td>0.164797</td>\n",
       "      <td>0.233035</td>\n",
       "      <td>0.304116</td>\n",
       "      <td>0.251319</td>\n",
       "      <td>0.261124</td>\n",
       "      <td>0.249533</td>\n",
       "      <td>0.244896</td>\n",
       "      <td>0.281999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.295431</td>\n",
       "      <td>0.183081</td>\n",
       "      <td>0.222160</td>\n",
       "      <td>0.320608</td>\n",
       "      <td>0.261499</td>\n",
       "      <td>0.309888</td>\n",
       "      <td>0.290684</td>\n",
       "      <td>0.258127</td>\n",
       "      <td>0.152506</td>\n",
       "      <td>0.274895</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.251769</td>\n",
       "      <td>0.235655</td>\n",
       "      <td>0.240658</td>\n",
       "      <td>0.210569</td>\n",
       "      <td>0.250817</td>\n",
       "      <td>0.245811</td>\n",
       "      <td>0.241521</td>\n",
       "      <td>0.240948</td>\n",
       "      <td>0.239477</td>\n",
       "      <td>0.245323</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.256584</td>\n",
       "      <td>0.252778</td>\n",
       "      <td>0.257748</td>\n",
       "      <td>0.267818</td>\n",
       "      <td>0.280131</td>\n",
       "      <td>0.265739</td>\n",
       "      <td>0.246184</td>\n",
       "      <td>0.249315</td>\n",
       "      <td>0.257751</td>\n",
       "      <td>0.245983</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.208672</td>\n",
       "      <td>0.260032</td>\n",
       "      <td>0.252899</td>\n",
       "      <td>0.244045</td>\n",
       "      <td>0.267450</td>\n",
       "      <td>0.291161</td>\n",
       "      <td>0.172832</td>\n",
       "      <td>0.203573</td>\n",
       "      <td>0.209098</td>\n",
       "      <td>0.289944</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         OBS_0     OBS_1     OBS_2     OBS_3     OBS_4     OBS_5     OBS_6  \\\n",
       "0     0.268372  0.246112  0.320277  0.157779  0.255752  0.174061  0.301679   \n",
       "1     0.241903  0.239770  0.258264  0.253273  0.243815  0.277775  0.223547   \n",
       "2     0.185326  0.223879  0.240827  0.249868  0.232626  0.242109  0.246553   \n",
       "3     0.233334  0.252164  0.278026  0.249751  0.236253  0.240901  0.245941   \n",
       "4     0.234331  0.234900  0.250295  0.248989  0.247575  0.252747  0.250756   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1495  0.213068  0.241627  0.164797  0.233035  0.304116  0.251319  0.261124   \n",
       "1496  0.295431  0.183081  0.222160  0.320608  0.261499  0.309888  0.290684   \n",
       "1497  0.251769  0.235655  0.240658  0.210569  0.250817  0.245811  0.241521   \n",
       "1498  0.256584  0.252778  0.257748  0.267818  0.280131  0.265739  0.246184   \n",
       "1499  0.208672  0.260032  0.252899  0.244045  0.267450  0.291161  0.172832   \n",
       "\n",
       "         OBS_7     OBS_8     OBS_9  separable  \n",
       "0     0.233190  0.208881  0.241814      False  \n",
       "1     0.237721  0.237809  0.241504      False  \n",
       "2     0.233927  0.234095  0.246902       True  \n",
       "3     0.248044  0.280448  0.269990      False  \n",
       "4     0.240557  0.257094  0.250061       True  \n",
       "...        ...       ...       ...        ...  \n",
       "1495  0.249533  0.244896  0.281999      False  \n",
       "1496  0.258127  0.152506  0.274895      False  \n",
       "1497  0.240948  0.239477  0.245323       True  \n",
       "1498  0.249315  0.257751  0.245983       True  \n",
       "1499  0.203573  0.209098  0.289944      False  \n",
       "\n",
       "[1500 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of total exemples \n",
    "n_exemples = 1500\n",
    "\n",
    "# Loading dataset with pandas\n",
    "dataset = pd.read_csv('../datasets/ds_mixed_obs.csv')\n",
    "dataset = dataset[:n_exemples]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features\n",
    "n_features = 8\n",
    "\n",
    "# Dividing dataset in features and label\n",
    "X = dataset.to_numpy()[:,0:n_features]\n",
    "y = dataset.to_numpy()[:,-1]\n",
    "\n",
    "# Scaling the inputs \n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Redifining labels on -1,1\n",
    "y_scaled = np.array(2 * (y - 0.5),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1200\n",
      "Test size: 300\n"
     ]
    }
   ],
   "source": [
    "# Spliting dataset in training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled,train_size=0.8)\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Qbits:  8\n"
     ]
    }
   ],
   "source": [
    "# Setting the number of qbits the same as the number of features\n",
    "n_qubits = len(X_train[0])\n",
    "print(\"Number of Qbits: \",n_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paolodarold/opt/anaconda3/envs/QML/lib/python3.9/site-packages/pennylane_lightning/lightning_qubit/lightning_qubit.py:901: UserWarning: Pre-compiled binaries for lightning.qubit are not available. Falling back to using the Python-based default.qubit implementation. To manually compile from source, follow the instructions at https://pennylane-lightning.readthedocs.io/en/latest/installation.html.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "dev_var = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev_var, diff_method=\"parameter-shift\")\n",
    "def quantum_model(x, params):\n",
    "    \"\"\"A variational quantum model.\"\"\"\n",
    "\n",
    "    # embedding\n",
    "    #IQPEmbedding(x, wires=range(n_qubits),n_repeats=2)\n",
    "    qml.AngleEmbedding(x, wires=range(n_qubits))\n",
    "\n",
    "\n",
    "    # trainable measurement\n",
    "    StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_model_plus_bias(x, params, bias):\n",
    "    \"\"\"Adding a bias.\"\"\"\n",
    "    return quantum_model(x, params) + bias\n",
    "\n",
    "def hinge_loss(predictions, targets):\n",
    "    \"\"\"Implements the hinge loss.\"\"\"\n",
    "    all_ones = torch.ones_like(targets)\n",
    "    hinge_loss = all_ones - predictions * targets\n",
    "    # trick: since the max(0,x) function is not differentiable,\n",
    "    # use the mathematically equivalent relu instead\n",
    "    hinge_loss = relu(hinge_loss)\n",
    "    return hinge_loss\n",
    "\n",
    "def quantum_model_train(n_layers, steps, batch_size):\n",
    "    \"\"\"Train the quantum model defined above.\"\"\"\n",
    "\n",
    "    params = np.random.random((n_layers, n_qubits, 3))\n",
    "    params_torch = torch.tensor(params, requires_grad=True)\n",
    "    bias_torch = torch.tensor(0.0)\n",
    "\n",
    "    opt = torch.optim.Adam([params_torch, bias_torch], lr=0.01)\n",
    "\n",
    "    loss_history = []\n",
    "    for i in trange(steps):\n",
    "\n",
    "        batch_ids = np.random.choice(len(X_train), batch_size)\n",
    "\n",
    "        X_batch = X_train[batch_ids]\n",
    "        y_batch = y_train[batch_ids]\n",
    "\n",
    "        X_batch_torch = torch.tensor(X_batch, requires_grad=False)\n",
    "        y_batch_torch = torch.tensor(y_batch, requires_grad=False)\n",
    "\n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            preds = torch.stack(\n",
    "                [quantum_model_plus_bias(x, params_torch, bias_torch) for x in X_batch_torch]\n",
    "            )\n",
    "            loss = torch.mean(hinge_loss(preds, y_batch_torch))\n",
    "\n",
    "            # bookkeeping\n",
    "            current_loss = loss.detach().numpy().item()\n",
    "            loss_history.append(current_loss)\n",
    "            if i % 10 == 0:\n",
    "                print(\"step\", i, \", loss\", current_loss)\n",
    "\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        opt.step(closure)\n",
    "\n",
    "    return params_torch, bias_torch, loss_history\n",
    "\n",
    "def quantum_model_predict(X_pred, trained_params, trained_bias):\n",
    "    \"\"\"Predict using the quantum model defined above.\"\"\"\n",
    "\n",
    "    p = []\n",
    "    for x in X_pred:\n",
    "\n",
    "        x_torch = torch.tensor(x)\n",
    "        pred_torch = quantum_model_plus_bias(x_torch, trained_params, trained_bias)\n",
    "        pred = pred_torch.detach().numpy().item()\n",
    "        if pred > 0:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = -1\n",
    "\n",
    "        p.append(pred)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edddf48386b84dc39fb0d9b9422d3239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , loss 0.9437426428800814\n",
      "step 10 , loss 0.9547223257626415\n"
     ]
    }
   ],
   "source": [
    "n_layers = 4\n",
    "batch_size = 24\n",
    "steps = 50\n",
    "trained_params, trained_bias, loss_history = quantum_model_train(n_layers, steps, batch_size)\n",
    "\n",
    "pred_test = quantum_model_predict(X_test, trained_params, trained_bias)\n",
    "print(\"accuracy on test set:\", accuracy_score(pred_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
